{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Web Scraping with Python Workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use the python package, Beautiful Soup, to webscrape headlines from the NY Times. By scraping the headlines, we wil examine how to search for meta data hidden within HTML tags and how HTML tags can be removed with data scraping.\n",
    "\n",
    "Following the exercise of web scraping on a static webapage, we will crawl a similar webpage and use the crawler to \"click\" on links embedded within the webpage.\n",
    "\n",
    "We will then store the data in a Pandas dataframe and show how to transfer this information to aa csv.\n",
    "\n",
    "Feel free to send questions to CDSS_executives@columbia.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import the Beautiful Soup package as well as urllib, a package that is used to process url's. If you don't have these packages, please install `python3` and `bs4` either with your favorite package manager or by means of `conda`. You will only be able to use the `urllib.request` functionality as shown in a Jupyter notebook running python3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find the search URL that you would like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "search_url = 'http://nytimes.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Form the Beautiful Soup query around the url. For more information, visit the Beautiful Soup documentation https://www.crummy.com/software/BeautifulSoup/bs4/doc/. For trying out different url's, start by modifying the \"search_url\" variable from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(urlopen(search_url).read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Beautiful Soup produces essentially copy of the html file from the web application. So let's print it to see what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It doesn't look too nice! We need to find a way to parse through it. Let's start by looking at the HTML identifiers near the information that we want -- the headlines. Let's command+F our printed soup output for the context of one of the headlines that we want.\n",
    "\n",
    "```\n",
    "<h2 class=\"story-heading\"><a href=\"https://www.nytimes.com/2017/02/01/arts/beyonce-pregnant-twins.html\">Beyoncé Announces She Is Pregnant With Twins</a></h2>\n",
    "<p class=\"byline\">By JOE COSCARELLI <time class=\"timestamp\" data-eastern-timestamp=\"2:49 PM\" data-utc-timestamp=\"1485978561\" datetime=\"2017-02-01\">2:49 PM ET</time></p>\n",
    "<p class=\"summary\">\n",
    "        The pop star shared an Instagram post in which she said her family with the rapper Jay Z “will be growing by two.”    </p>\n",
    "```\n",
    "\n",
    "This headline is denoted by the tag \"< h2 >\" snd the class \"story-heading\". With further examination with more command+F searches through the soup output, we confirm that \"story-heading\" is used to denote the headline for all of the stories on the homepage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " soup2 = soup.findAll('h2', {'class':'story-heading'})\n",
    " print(soup2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we just need to scrape away the html tags to get the text that we want. Since not every line in soup2 has text at all, we first need to check for empty lines before getting the text element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for line in soup2:\n",
    "    if line:\n",
    "        lines.append(line.text)\n",
    "        print(line.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How many headlines did we scrape? Does this number seem reasonable? Let's do some visual inspection of the data that we scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we've confirmed that our data looks like headlines, let's strip away all the \\n, trailing or leading whitespace and other characters that we don't want. We will do this using the string.strip() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "headlines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    headlines.append(line)\n",
    "print(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hmmmm...what seems to be going on here? The newline characters are still there! This is because they're in the middle of the strings and we only stripped whitespace characters from the ends of the line. Since it seems like certain headlines are copied before and after a series of \\n's, lets string.split() the lines, remove the trailing and leading whitespaces from the first element of the split string array, and use this processed string as our headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for line_index in range(0, len(headlines)):\n",
    "    first_elem = headlines[line_index].split('\\n')[0]\n",
    "    first_elem = first_elem.strip()\n",
    "  #  print(first_elem)\n",
    "    headlines[line_index] = first_elem\n",
    "print(headlines)\n",
    "   # print(headlines[line_index].split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's store the data in a useful way. How about a dataframe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(headlines)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That was simple, right? We can easily write this data to a CSV from pandas also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('headlines.csv')\n",
    "#make sure that it's there...\n",
    "#you can always use \"!\" as an escape character to use terminal commands in ipython notebooks\n",
    "!ls\n",
    "!head headlines.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's crawl (but not really crawling)! Let's try to store some text from Reuters articles. This has debatable legality so do this at your own risk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reuters_url = 'http://www.reuters.com/'\n",
    "crawl_soup = BeautifulSoup(urlopen(reuters_url).read(), 'html.parser')\n",
    "print(crawl_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Similarly to the NYT, Reuters stories are mostly identified using the \"story-title\" class, but we want the url's this time! The url is stored next to the story-title. The href= URL will be stored as a property once we find the story-title classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "crawl_soup1 = crawl_soup.findAll(True, {'class':'story-title'})\n",
    "#print(crawl_soup1)\n",
    "\n",
    "#now get the href, note that it always starts with <a href=\"....\n",
    "#remember that lines can be null\n",
    "\n",
    "r_headlines = []\n",
    "#hacky way when soup tags don't work\n",
    "#use escape character \"\\\" to split\n",
    "for line in crawl_soup1:\n",
    "    if len(line)>1:\n",
    "       for i in str(line).split(\"\\\"\"):\n",
    "            if 'article' in i:\n",
    "                #print(i)\n",
    "                r_headlines.append(i)\n",
    "                \n",
    "print(r_headlines)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
