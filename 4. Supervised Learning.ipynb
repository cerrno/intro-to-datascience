{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Random Forests\n",
    "\n",
    "Supervised learning is a machine learning problem where you can train a model to use features to predict a target variable based on prior examples.  This contrasts with unsupervised learning (eg. clustering), in which the data contains many features but no apparent target variable.\n",
    "\n",
    "To get a taste for supervised learning, we'll build a random forest model to predict secondary school student behavior using each student's attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is supervised learning?\n",
    "\n",
    "In machine learning, you have a problem that needs to be solved, data related to the problem, and a computer.  In supervised learning, you're feeding data to the computer and training it to output the correct answer for a given set of inputs.\n",
    "\n",
    "> For example, let's say you love eating at Jin Ramen but hate waiting for a seat.  Let's also suppose that you have a dataset of 500 visits to Jin Ramen that contains information about the wait time for a table, the time of arrival, day of week, number of people in the party, etc.  You could train a supervised learning model on this dataset to predict wait time based on the other variables.  In the future, whenever you want ramen, you can input the values you know (the \"features\") into this model to predict wait time (the \"target\"), then decide whether to visit Jin Ramen or not.\n",
    "\n",
    "Supervised learning usually involves regression and classification tasks:\n",
    "- Regression: the target variable is continuous (eg. predicting the minutes you'll wait for a table)\n",
    "- Classification: the target variable is a discrete value (eg. your waiting limit is 10 mins, so you train the model to output \"0\" if the wait time will be over 10 mins, and \"1\" if the wait time will be 10 mins or under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to random forest\n",
    "\n",
    "Random forest is a machine learning technique used for classification and regression tasks.  Random forest is an \"ensemble\" method because one model is actually composed of many decision trees, each of which produces an output; these outputs are then averaged to produce the random forest model's final prediction.\n",
    "\n",
    "### 2a. What's a decision tree?\n",
    "A decision tree is a model that predicts the target value by running the inputs through a tree structure.  Decision trees can perform regression and classification tasks, so they're pretty flexible when it comes to machine learning tasks.\n",
    "\n",
    "This image is a simple decision tree for predicting whether a Titanic passenger \"survived\" or \"died\" (those are the target values in this classification task):\n",
    "![decision tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
    "\n",
    "### 2b. Lots of decision trees = Forest\n",
    "A random forest model is composed of many (you decide how many) decision trees.  The idea is that one decision tree may not be accurate, but an entire forest of independent trees will provide higher accuracy because noise is canceled out and signal is strengthened.\n",
    "\n",
    "When creating each tree, the random forest algorithm takes a sample of the data (a bootstrap sample), and produces the best possible decision tree for that sample.  When building the tree, at every node/split, a random sample of features is used.  All this sample-taking allows the forest of trees to identify data features that are truly important, not arbitrary.\n",
    "\n",
    "Once the entire forest is created and ready to make predictions, the random forest model runs the input down every tree, and outputs the average or majority decision of its trees.\n",
    "\n",
    "> Pros:\n",
    "- High accuracy\n",
    "- Fast to train\n",
    "- Can handle many input variables, which relieves you of having to choose a subset of all variables before training the model\n",
    "- Can work with numeric and categorical variables\n",
    "\n",
    "> Cons:\n",
    "- Hard to interpret\n",
    "- Prone to overfitting on noisy datasets\n",
    "- Cannot deal with features or target values that do not exist in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random forest example\n",
    "We're going to predict student alcohol consumption using a dataset about Portuguese students.  Take a moment to skim over the variables [here](http://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION).\n",
    "\n",
    "For the training the random forest model, we're going to use the built-in functions of `scikit-learn`.  Documentation for its random forest classifier is [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data from the datasets folder\n",
    "student_port = pd.read_csv('datasets/student-por.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the first 5 rows\n",
    "student_port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset's dimensions\n",
    "student_port.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict alcohol consumption based on all or most of other variables (a classification problem).\n",
    "\n",
    "First, we need to perform some data cleaning (highly important for any data task!).  Off the bat, we can see that some of the columns don't have the correct data type.  We want to make sure every column is correctly stored as a numeric or categorical data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What's the current data type of each variable?\n",
    "student_port.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most variables should be categorical, so let's list out the variables that are numeric and/or nominal\n",
    "vars_num = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', \n",
    "            'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put all the rf data into a new df, rf_data\n",
    "rf_data = student_port[vars_num].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But need to convert every categorical value into a number\n",
    "# Label encoder converts to 0, 1, ... for alphabetically-sorted string values\n",
    "for col in student_port.columns:\n",
    "    if col not in vars_num:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        rf_data.loc[:, col] = le.fit_transform(student_port.loc[:, col])\n",
    "        rf_data.loc[:,col] = rf_data.loc[:, col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to know the string value corresponding to a numerical category in X, go back to student_port\n",
    "sorted(student_port['address'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many students in each category of workday alcohol consumption?\n",
    "student_port['Dalc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many students in each category of weekend alcohol consumption?\n",
    "student_port['Walc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five categories of student alcohol consumption may be too many to work with (especially since most students fall on the low end of consumption).  Since it's more interesting to divide students into light drinkers and heavy drinkers, let's create two categories of alcohol usage: 1-2 and 3-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map out the old level to the new level\n",
    "# 0 will be \"light drinkers\", 1 will be \"heavy drinkers\"\n",
    "alc_mapping = {1: 0, 2: 0, 3: 1, 4: 1, 5:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for the mapped data\n",
    "rf_data['Dalc_mapped'] = rf_data['Dalc'].map(alc_mapping)\n",
    "rf_data['Walc_mapped'] = rf_data['Walc'].map(alc_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling time!\n",
    "\n",
    "Supervised learning involves these general steps:\n",
    "\n",
    "1. Split the data into training and test datasets.  Usually the split is anywhere from 60%/40% to 80%/20%.\n",
    "2. Train a model on the training set.\n",
    "3. Apply the model on the test set and compare the predicted results against the actual values to evaluate the model's performance.\n",
    "    - As needed, iterate on the model to improve its performance (beware of overfitting!)\n",
    "    - Look into the model to understand what it's doing, and gain some insight on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Decide on the data you want, and split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want as features and as the target\n",
    "features = rf_data.columns\n",
    "features = features.drop(['G1', 'G2', 'Mjob', 'Fjob', 'Dalc', 'Walc', 'Dalc_mapped', 'Walc_mapped'])\n",
    "target = 'Walc_mapped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the full dataset of predictor variables\n",
    "X = rf_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the column of target (to be predicted) variables\n",
    "y = rf_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (70%) and test (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Train the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random forest model with your desired parameters\n",
    "rf_model = RandomForestClassifier(n_estimators = 100, random_state = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy: the number of correct predictions divided by the number of predictions\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test data to produce predictions\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix helps you evaluate the predictions (rows) against the actual values (columns)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pretty visualization of the confusion matrix\n",
    "# From http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred), classes = sorted(y.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features did the model deem most \"important\" for predicting the target variable?\n",
    "\n",
    "*Note:* Random forest models are considered \"black box\" models because they're hard to interpret.  If our model only had one decision tree instead of an entire forest of them, then we could examine the tree and see the exact criteria used to produce a prediction for any input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show feature importances, highest first\n",
    "pd.DataFrame({'feature': X.columns, 'importance': rf_model.feature_importances_}).sort_values(\n",
    "    by = 'importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, does it make sense that the top few features are most important in determining a student's alcohol level consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Next steps: Different ways to play around with random forests*\n",
    "\n",
    "- Tweak the random forest initialization parameters: change the number of decision trees\n",
    "- Use a different set of features in X\n",
    "- Predict a different variable (eg. student's test scores)\n",
    "    - Use RandomForestRegressor instead of RandomForestClassifier to predict a variable that is numeric and continuous rather than categorical\n",
    "- Transform a variable in a different way (eg. a different mapping of variables; combine variables with each other)\n",
    "- Use random forests's built-in out-of-bag error to evaluate model performance\n",
    "    - Out-of-bag estimate is the error rate of the random forest model on the training data that is not included in the bootstrap sample of each tree.  Oob error has been shown to be a good measure of error for random forest models.\n",
    "    - When initializing `RandomForestClassifier`, set `oob_score = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further resources\n",
    "\n",
    "- Another random forest tutorial: [Random Forests in Python](http://blog.yhat.com/posts/random-forests-in-python.html)\n",
    "- [Would You Survive the Titanic? A Guide to Machine Learning in Python](https://blog.socialcops.com/engineering/machine-learning-python/)\n",
    "- [Supervised learning with scikit-learn](http://scikit-learn.org/stable/supervised_learning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
